# Loading the libraries
library(tm)
library(caret)
library(SnowballC)
library(kernlab)
library(e1071)
library(LiblineaR)
library(naivebayes)

# Loading the data
training <- read.csv("/training.csv", sep=',', dec=',', stringsAsFactors=FALSE)
testing <- read.csv("/test.csv", sep=',', dec=',')

# Transform the target variable from character to factor
training$airline_sentiment = as.factor(training$airline_sentiment)
levels(training$airline_sentiment)

# Randomising the dataset to facilitate the training process
set.seed(789)
training <- training[sample(nrow(training)), ]
training <- training[sample(nrow(training)), ]

# Read the text 
corpus <- Corpus(VectorSource(training$text))

# Data cleaning
cleanCorpus <- function(corpus) {
  corpus <-tm_map(corpus, stemDocument)
  corpus.tmp <- tm_map(corpus,removePunctuation)
  corpus.tmp <- tm_map(corpus.tmp,stripWhitespace)
  corpus.tmp <- tm_map(corpus.tmp,removeWords,stopwords("en"))
  return(corpus.tmp)
}

# Now we clean the corpus
corpus.clean <- cleanCorpus(corpus)

# Represent the clean corpus with a document term matrix (DTM)
dtm <- DocumentTermMatrix(corpus.clean,control = list(weighting= function(x) weightBin(x)))
dtm <- removeSparseTerms(dtm, .99)

# Cross-validation: we split our training dataset (once cleaned) into train (75%) and test (25%)
nrow(training)  # there are 7000 rows

data.train <- training[1:5250,]
data.test <- training[5251:7000,]

tweet_id <- data.test[,"tweet_id"]

dtm.train <- dtm[1:5250,]
dtm.test <- dtm[5251:7000,]

corpus.clean.train <- corpus.clean[1:5250]
corpus.clean.test <- corpus.clean[5251:7000]

# Creating a new train and test datasets (cross-validation) based on DTM clean corpus
# Transform the DRM train data into a matrix to better manipulate the data and store the matrix in a variable x
x <- as.matrix(dtm.train)
# Store the target variable in a new variable for later use
y <- data.train$airline_sentiment

# Create a new train and test datasets for cross-validation: train_data is a dataframe combination of the DTM clean corpus 
# and the target variable, and test_Data is the DTM clean corpus but saved as a dataframe
train_data <- as.data.frame(cbind(y,x))
test_data <- as.data.frame(as.matrix(dtm.test))

# Create an SVM model for train_data using the "e1071" package
sv <- svm(y~., train_data, type="C-classification", kernel="sigmoid", cost=1)

## Evaluation of the SVM model 

# Predict and compute the confusion matrix 
prediction <- predict(sv, test_data)
table("Predictions"= prediction,  "Actual"=data.test$airline_sentiment)

# Calculate the accuracy of the model from the confusion matrix
acc <- function(table){
  TP = table[1,1];  # true positives
  TN = table[2,2];  # true negatives
  FP = table[1,2];  # false positives
  FN = table[2,1];  # false negatives
  acc = (TP + TN)/(TP + TN + FP + FN)
  return(acc)
}
accuracy <- acc(table("Predictions"= prediction, "Actual"=data.test$airline_sentiment))
paste0("The accuracy rate of this model is " ,
       accuracy)

# We want to improve the accuracy rate of 0.82 by tuning the SVM parameters, however we get a slightly lower accuracy rate of 0.81
fitControl <- trainControl(method = "cv",
                           number = 5,
                           verboseIter = TRUE)

cv.svm <- train(x,y,
                method="svmRadial",
                preProc = c("center", "scale"),
                tuneLength = 5,
                metric = "Accuracy",
                trControl = fitControl)

cv.svm.prediction <- predict(cv.svm, test_data)
table("Predictions"= cv.svm.prediction, "Actual" =data.test$airline_sentiment)

accuracy1 <- acc(table("Predictions"= cv.svm.prediction, "Actual"=data.test$airline_sentiment))

paste0("The accuracy rate of this model is " ,
       accuracy1)

# Try using a different model, Naive Bayes, but the accuracy rate (0.81) is slightly lower than the one we got with the SVM model 
# (before tuning the parameters of the model) (0.82)
# Feature selection
dim(dtm.train)
dtm.train.nb <- removeSparseTerms(dtm.train, 0.99)
dtm.test.nb <- removeSparseTerms(dtm.test, 0.99)
# Convert the word frequency to binary 1/0 presence
binarize <- function(data) {
  factor(ifelse(data > 0, 1,0), levels=c(0,1), labels=c("Yes", "No"))
}
# Apply the function to training and test data
dtm.train.nb.binary <- apply(dtm.train.nb, 2, binarize)
dtm.test.nb.binary <- apply(dtm.test.nb, 2, binarize)
# Naive bayes model
nb_model <- naiveBayes(dtm.train.nb.binary, data.train$airline_sentiment, laplace = 1)
probs <- predict(nb_model, newdata=dtm.test.nb.binary, type = "raw")
classes <- predict(nb_model, newdata=dtm.test.nb.binary, type = "class")

# Model evaluation 
# Confusion matrix
table("Predictions"= classes,  "Actual" = data.test$airline_sentiment)
# Compute the accuracy rate using the confusion matrix
acc <- function(table){
  TP = table[1,1];  # true positives
  TN = table[2,2];  # true negatives
  FP = table[1,2];  # false positives
  FN = table[2,1];  # false negatives
  acc = (TP + TN)/(TP + TN + FP + FN)
  return(acc)
}
accuracy2 <- acc(table("Predictions"= classes,  "Actual" =data.test$airline_sentiment))
paste0("The accuracy rate of this model is " ,
       accuracy2)

# Generate the output CSV file using out best model (SVM)
# Rename the levels of prediction 
prediction1 <- as.data.frame(prediction)
levels(prediction1[,1])[c(1,2,3)] = c("negative","neutral","positive")

# Add the tweet_id column to the predictions
prediction2 <- cbind(tweet_id, prediction1)
colnames(prediction2) <- c("tweet_id","airline_sentiment")

# Write the output to a CSV file that matches the desired output
write.csv(prediction2, file = "TaniaElAchkar_output.csv", row.names=F)
